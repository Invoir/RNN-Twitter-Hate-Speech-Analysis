{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Analysis with LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll apply deep learning technique LSTM (long short-term memory units) to the task of hate speech analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framing Hate Speech Analysis as a Deep Learning Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of sentiment analysis involves taking in an input sequence of words and determining whether the tweet is hate speech, offensive language or neither . We can separate this into 5 different components.\n",
    "\n",
    "    1) Training a word vector generation model (such as Word2Vec) or loading pretrained word vectors\n",
    "    2) Creating an ID's matrix for our training set\n",
    "    3) RNN (With LSTM units)\n",
    "    4) Training \n",
    "    5) Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to create our word vectors. For simplicity, we're going to be using a pretrained model. \n",
    "\n",
    "As one of the biggest players in the ML game, Google was able to train a Word2Vec model on a massive Google News dataset that contained over 100 billion different words! From that model, Google [was able to create 3 million word vectors](https://code.google.com/archive/p/word2vec/#Pre-trained_word_and_phrase_vectors), each with a dimensionality of 300. \n",
    "\n",
    "In an ideal scenario, we'd use those vectors, but since the word vectors matrix is quite large (3.6 GB!), we'll be using a much more manageable matrix that is trained using [GloVe](http://nlp.stanford.edu/projects/glove/), a similar word vector generation model. The matrix will contain 400,000 word vectors, each with a dimensionality of 50. \n",
    "\n",
    "We're going to be importing two different data structures, one will be a Python list with the 400,000 words, and one will be a 400,000 x 50 dimensional embedding matrix that holds all of the word vector values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('./data/wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('./data/wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure everything has been loaded in correctly, we can look at the dimensions of the vocabulary list and the embedding matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search our word list for a word like \"baseball\", and then access its corresponding vector through the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9327  ,  1.0421  , -0.78515 ,  0.91033 ,  0.22711 , -0.62158 ,\n",
       "       -1.6493  ,  0.07686 , -0.5868  ,  0.058831,  0.35628 ,  0.68916 ,\n",
       "       -0.50598 ,  0.70473 ,  1.2664  , -0.40031 , -0.020687,  0.80863 ,\n",
       "       -0.90566 , -0.074054, -0.87675 , -0.6291  , -0.12685 ,  0.11524 ,\n",
       "       -0.55685 , -1.6826  , -0.26291 ,  0.22632 ,  0.713   , -1.0828  ,\n",
       "        2.1231  ,  0.49869 ,  0.066711, -0.48226 , -0.17897 ,  0.47699 ,\n",
       "        0.16384 ,  0.16537 , -0.11506 , -0.15962 , -0.94926 , -0.42833 ,\n",
       "       -0.59457 ,  1.3566  , -0.27506 ,  0.19918 , -0.36008 ,  0.55667 ,\n",
       "       -0.70315 ,  0.17157 ], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseballIndex = wordsList.index('baseball')\n",
    "wordVectors[baseballIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our vectors, our first step is taking an input sentence and then constructing the its vector representation. Let's say that we have the input sentence \"I thought the movie was incredible and inspiring\". In order to get the word vectors, we can use Tensorflow's embedding lookup function. This function takes in two arguments, one for the embedding matrix (the wordVectors matrix in our case), and one for the ids of each of the words. The ids vector can be thought of as the integerized representation of the training set. This is basically just the row index of each of the words. Let's look at a quick example to make this concrete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[    41    804 201534   1005     15   7446      5  13767      0      0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "maxSeqLength = 10 #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "#firstSentence[8] and firstSentence[9] are going to be 0\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10 x 50 output should contain the 50 dimensional word vectors for each of the 10 words in the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create the ids matrix now, code is commented and instead using pregenrated idx.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('./data/cleanedData.csv')\n",
    "tweets = df.values\n",
    "\n",
    "max_tweet_length = 30 # Max word count of tweet\n",
    "\n",
    "#Words to tokens\n",
    "ids = np.zeros((tweets.shape[0], max_tweet_length), dtype='int32')\n",
    "#\n",
    "#for i, tweet in enumerate(tweets):\n",
    "#    text = tweet[6]\n",
    "#\n",
    "#    # Text cleaning\n",
    "#    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text)\n",
    "#\n",
    "#    split_text = text.split()\n",
    "#\n",
    "#    # Tokenize text\n",
    "#    for j, word in enumerate(split_text):\n",
    "#        try:\n",
    "#           ids[i][j] = wordsList.index(word)\n",
    "#        except ValueError:\n",
    "#            ids[i][j] = 399999 # Vector for unkown words\n",
    "#        if j == max_tweet_length - 1:\n",
    "#            break\n",
    "            \n",
    "#np.save('./data/ids_matrix', ids)\n",
    "\n",
    "ids = np.load('./data/ids_matrix.npy')\n",
    "\n",
    "\n",
    "# Convert tokens to word vectors using Glove's 50 dimension pre-trained word vectors\n",
    "# this to be used in next step\n",
    "word_vec_dimension = 50\n",
    "word_vectors = np.load('./data/wordVectors.npy')\n",
    "\n",
    "output_labels = []\n",
    "for tweet in tweets:\n",
    "    label = tweet[5]\n",
    "    if label == 0:\n",
    "        label = [1, 0, 0]\n",
    "    if label == 1:\n",
    "        label = [0, 1, 0]\n",
    "    if label == 2:\n",
    "        label = [0, 0, 1]\n",
    "    output_labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we’re ready to start creating our Tensorflow graph. We’ll first need to define some hyperparameters, such as batch size, number of LSTM units, number of output classes, and number of training iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "(16000, 30)\n",
      "2\n",
      "rt mention a a woman you shouldnt complain about cleaning up your house a a man you should always take the trash out\n"
     ]
    }
   ],
   "source": [
    "#batchSize = 24\n",
    "#lstmUnits = 64\n",
    "#numClasses = 2\n",
    "#iterations = 100000\n",
    "\n",
    "# Model hyperparameters\n",
    "output_classes = 3\n",
    "lstm_units = 64\n",
    "\n",
    "data_size = 19999\n",
    "batch_size = 33\n",
    "iterations = 100000\n",
    "\n",
    "training_ids, training_labels = ids[:16000], output_labels[:16000]\n",
    "testing_ids, testing_labels = ids[16000:], output_labels[16000:]\n",
    "\n",
    "def get_train_batch():\n",
    "    global batch_index, tweets\n",
    "\n",
    "    arr = np.zeros([batch_size, max_tweet_length])\n",
    "    labels = []\n",
    "\n",
    "    for tweet in range(batch_size):\n",
    "        num = random.randint(0, training_ids.shape[0]-1)\n",
    "\n",
    "        arr[tweet] = training_ids[num]\n",
    "\n",
    "        label = tweets[num][5]\n",
    "        if label == 0: # Hate speech\n",
    "            label = [1, 0, 0]\n",
    "        if label == 1: # Offensive language\n",
    "            label = [0, 1, 0]\n",
    "        if label == 2: # Neither\n",
    "            label = [0, 0, 1]\n",
    "        labels.append(label)\n",
    "\n",
    "    return arr, labels\n",
    "\n",
    "#Check training data\n",
    "print(len(training_ids))\n",
    "print(training_ids.shape)\n",
    "df.head(10)\n",
    "print(tweets[0][5])\n",
    "print(tweets[0][6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "input_data = tf.compat.v1.placeholder(tf.int32, [batch_size, max_tweet_length], name=\"Tweets\")\n",
    "labels = tf.compat.v1.placeholder(tf.float32, [batch_size, output_classes], name=\"Labels\")\n",
    "\n",
    "data = tf.Variable(tf.zeros([batch_size, max_tweet_length, word_vec_dimension]), dtype=tf.float32, name=\"InputData\")\n",
    "data = tf.nn.embedding_lookup(word_vectors, input_data)\n",
    "\n",
    "lstm_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(lstm_units)\n",
    "lstm_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell=lstm_cell, output_keep_prob=0.75)\n",
    "value, _ = tf.compat.v1.nn.dynamic_rnn(lstm_cell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.compat.v1.truncated_normal([lstm_units, output_classes]), name=\"Weights\")\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[output_classes]), name=\"Bias\")\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard setup\n",
    "tf.compat.v1.summary.scalar('Loss', loss)\n",
    "tf.compat.v1.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.compat.v1.summary.merge_all()\n",
    "logdir = \"./tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.compat.v1.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to models/pretrained_lstm.ckpt-10000 - 10 %\n",
      "Saved to models/pretrained_lstm.ckpt-20000 - 20 %\n",
      "Saved to models/pretrained_lstm.ckpt-30000 - 30 %\n",
      "Saved to models/pretrained_lstm.ckpt-40000 - 40 %\n",
      "Saved to models/pretrained_lstm.ckpt-50000 - 50 %\n",
      "Saved to models/pretrained_lstm.ckpt-60000 - 60 %\n",
      "Saved to models/pretrained_lstm.ckpt-70000 - 70 %\n",
      "Saved to models/pretrained_lstm.ckpt-80000 - 80 %\n",
      "Saved to models/pretrained_lstm.ckpt-90000 - 90 %\n",
      "Saved to models/pretrained_lstm.ckpt-100000 - 100 %\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "for i in range(iterations + 1):\n",
    "    batch, batch_labels = get_train_batch()\n",
    "    #print(len(batch))\n",
    "    #print(batch.shape)\n",
    "    sess.run(optimizer, {input_data: batch, labels: batch_labels})\n",
    "\n",
    "    # Write summary to Tensorboard every 50 epochs\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {input_data: batch, labels: batch_labels})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    # Save the network every 10,000 epochs\n",
    "    if (i % 10000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"Saved to\", save_path, \"-\", int(i/iterations*100), \"%\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-100000\n",
      "Accuracy on validation data: 0.8881940970485243\n"
     ]
    }
   ],
   "source": [
    "# Model prediction\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))\n",
    "\n",
    "def get_sentence_matrix(sentence):\n",
    "    arr = np.zeros([batch_size, max_tweet_length])\n",
    "    sentence_matrix = np.zeros([batch_size, max_tweet_length], dtype='int32')\n",
    "    split = sentence.split()\n",
    "    for i, word in enumerate(split):\n",
    "        if (i < max_tweet_length):\n",
    "            try:\n",
    "                sentence_matrix[0, i] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                sentence_matrix[0, i] = 399999 #Vector for unkown words\n",
    "    return sentence_matrix\n",
    "\n",
    "correct = 0\n",
    "for i in range(3998):\n",
    "    input_matrix = get_sentence_matrix(tweets[16000 + i][6])\n",
    "    predicted = sess.run(prediction, {input_data: input_matrix})[0]\n",
    "    if (np.argmax(predicted) == tweets[16000 + i][5]):\n",
    "        correct += 1\n",
    "\n",
    "print(\"Accuracy on validation data:\", correct/3998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we went over a deep learning approach to hate speech analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
